{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Qinue8j-Nx"
      },
      "source": [
        "# Simple Benchmarker\n",
        "\n",
        "This is running on Google's free cloud compute. This notebook must be kept open for the code to keep running.\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. **To run this notebook, you must have a google account and be using Chrome browser**.\n",
        "\n",
        "  a. Notebooks can be ran continuously for 12 hours on Google Colabs before a reset is required\n",
        "\n",
        "2. Run the code below (press Ctrl + F9, or press the play button next to \"Show code\" below)\n",
        "\n",
        "    a. First time running this notebook, a popup will appear warning about non-Google authored code. Please be assured that none of our code will ever request access to your Google account.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5u4h6wtLj-N1"
      },
      "outputs": [],
      "source": [
        "#@title script to start Simple Benchmarker\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def cloneOrPullTIGRepo(repo_name):\n",
        "    repo_path = f\"{os.getcwd()}/{repo_name}\"\n",
        "    if not os.path.exists(repo_path):\n",
        "        subprocess.run([\"git\", \"clone\", \"-b\", \"master\", f\"https://github.com/the-innovation-game/{repo_name}.git\"], check=True)\n",
        "    else:\n",
        "        subprocess.run([\"git\", \"pull\", \"origin\", \"master\"], cwd=repo_path, check=True)\n",
        "    if repo_path not in sys.path:\n",
        "        sys.path.append(repo_path)\n",
        "\n",
        "cloneOrPullTIGRepo(\"simple_benchmarker\")\n",
        "cloneOrPullTIGRepo(\"challenges\")\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from benchmarker_utils.api import API\n",
        "from benchmarker_utils.misc import logger, calcSeed, randomInterpolate, IntermediateIntegersLogger\n",
        "from threading import Thread\n",
        "from uuid import uuid4\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "if 'G' not in locals():\n",
        "  G = {}\n",
        "\n",
        "# Start Benchmarker\n",
        "if \"BENCHMARKER\" not in G:\n",
        "  class SimpleBenchmarker:\n",
        "      def __init__(self, api: API):\n",
        "          self.api = api\n",
        "          self.running = True\n",
        "          self.status = \"Initialising\"\n",
        "          self.earnings = None\n",
        "          self.active_algorithms = {}\n",
        "          self.recent_benchmarks = None\n",
        "          self.underperformed_benchmarks = {\n",
        "            'header_row': [\n",
        "                'id',\n",
        "                'datetime_submitted',\n",
        "                'latest_earnings',\n",
        "                'block_id',\n",
        "                'challenge_id',\n",
        "                'algorithm_id',\n",
        "                'difficulty',\n",
        "                'num_solutions',\n",
        "                'frontier_idx',\n",
        "                'status',\n",
        "            ],\n",
        "            'data_rows': []\n",
        "          }\n",
        "          self.latest_block = None\n",
        "          self.frontiers = None\n",
        "          self.reset()\n",
        "\n",
        "      def reset(self):\n",
        "          self.proofs = []\n",
        "          self.nonce = 0\n",
        "          self.num_errors = 0\n",
        "          self.benchmark_start = None\n",
        "          self.benchmark_end = None\n",
        "          self.challenge_id = None\n",
        "          self.algorithm_id = None\n",
        "          self.difficulty = None\n",
        "\n",
        "      def pickChallengeToBenchmark(self):\n",
        "          # picks challenge which we have benchmarked the least\n",
        "          # this helps us increase our balance score\n",
        "          min_earnings = min(\n",
        "              c[1]\n",
        "              for c in self.earnings[\"benchmarker_earnings\"][\"latest_by_challenge\"]\n",
        "          )\n",
        "          return random.choice([\n",
        "              c[0]\n",
        "              for c in self.earnings[\"benchmarker_earnings\"][\"latest_by_challenge\"]\n",
        "              if c[1] == min_earnings\n",
        "          ])\n",
        "\n",
        "      def pickAlgorithmToBenchmark(self):\n",
        "          algorithm_ids = [a[0] for a in self.active_algorithms[self.challenge_id]['data_rows']]\n",
        "          return random.choice(algorithm_ids)\n",
        "\n",
        "      def pickDifficultyToBenchmark(self):\n",
        "          benchmarks = [\n",
        "              {\n",
        "                  k: d[i]\n",
        "                  for i, k in enumerate(self.frontiers[\"header_row\"])\n",
        "              }\n",
        "              for d in self.frontiers[\"data_rows\"]\n",
        "          ]\n",
        "\n",
        "          # group benchmarks by frontier_idx\n",
        "          benchmarks_by_frontier_idx = {}\n",
        "          num_solutions_on_frontiers = 0\n",
        "          for b in benchmarks:\n",
        "              benchmarks_by_frontier_idx.setdefault(b[\"frontier_idx\"], []).append(b)\n",
        "              num_solutions_on_frontiers += b[\"num_solutions\"] * (b[\"frontier_idx\"] is not None)\n",
        "          # LOGGER.debug(f\"Got {len(benchmarks)} benchmarks with {num_solutions_on_frontiers} solutions on frontiers\")\n",
        "\n",
        "          # FIXME this assumes Difficulty has exactly 2 parameters\n",
        "          difficulty_bounds = self.latest_block[\"difficulty_bounds\"][self.challenge_id]\n",
        "          x_param, y_param = list(difficulty_bounds)\n",
        "          min_difficulty = {x_param: difficulty_bounds[x_param][0], y_param: difficulty_bounds[y_param][0]}\n",
        "          max_difficulty = {x_param: difficulty_bounds[x_param][1], y_param: difficulty_bounds[y_param][1]}\n",
        "\n",
        "          # benchmarks with frontier_idx None do not earn tokens\n",
        "          # 0 is the easiest difficulty frontier\n",
        "          if 0 not in benchmarks_by_frontier_idx:\n",
        "              difficulty = min_difficulty\n",
        "          else:\n",
        "              # randomly interpolate a point on the easiest frontier\n",
        "              random_x, random_y = randomInterpolate(\n",
        "                  points=[(b[\"difficulty\"][x_param], b[\"difficulty\"][y_param]) for b in benchmarks_by_frontier_idx[1]],\n",
        "                  min_point=(min_difficulty[x_param], min_difficulty[y_param])\n",
        "              )\n",
        "              # randomly increment/decrement difficulty\n",
        "              pos_or_neg = (-1) ** (num_solutions_on_frontiers < self.latest_block[\"target_num_solutions\"]) # hack to set True=-1, False=1\n",
        "              difficulty = {\n",
        "                  x_param: random_x + random.randint(0, 1) * pos_or_neg,\n",
        "                  y_param: random_y + random.randint(0, 1) * pos_or_neg\n",
        "              }\n",
        "\n",
        "              for param in [x_param, y_param]:\n",
        "                # Ensure our difficulty is within the bounds\n",
        "                difficulty[param] = int(np.clip(\n",
        "                    difficulty[param],\n",
        "                    a_min=min_difficulty[param],\n",
        "                    a_max=max_difficulty[param]\n",
        "                ))\n",
        "          return difficulty\n",
        "\n",
        "      def generateAndSolveInstance(self, benchmark_params):\n",
        "          Challenge = __import__(f\"{self.challenge_id}.challenge\").challenge.Challenge\n",
        "          solveChallenge = getattr(\n",
        "              __import__(f\"{self.challenge_id}.algorithms.{self.algorithm_id}\").algorithms,\n",
        "              self.algorithm_id\n",
        "          ).solveChallenge\n",
        "\n",
        "          seed = calcSeed(**benchmark_params, nonce=self.nonce)\n",
        "          try:\n",
        "              logger = IntermediateIntegersLogger()\n",
        "              c = Challenge.generateInstance(seed, self.difficulty)\n",
        "              solution = solveChallenge(c, logger.log)\n",
        "              if c.verifySolution(solution):\n",
        "                  self.proofs.append(dict(\n",
        "                      nonce=self.nonce,\n",
        "                      solution=solution if isinstance(solution, list) else solution.tolist(),\n",
        "                      intermediate_integers=logger.dump()\n",
        "                  ))\n",
        "          except:\n",
        "              self.num_errors += 1\n",
        "          self.nonce += 1\n",
        "\n",
        "      def run_once(self):\n",
        "          self.status = \"Querying latest block\"\n",
        "          self.latest_block = self.api.getLatestBlock()\n",
        "          time.sleep(0.5)\n",
        "\n",
        "          self.status = \"Querying player earnings\"\n",
        "          self.earnings = self.api.getEarnings()\n",
        "          time.sleep(0.5)\n",
        "\n",
        "          self.status = \"Querying player benchmarks\"\n",
        "          self.recent_benchmarks = self.api.getRecentBenchmarks()\n",
        "          time.sleep(0.5)\n",
        "\n",
        "          self.status = \"Picking challenge to benchmark\"\n",
        "          self.challenge_id = self.pickChallengeToBenchmark()\n",
        "          time.sleep(0.5)\n",
        "\n",
        "          self.status = \"Querying active algorithms\"\n",
        "          self.active_algorithms[self.challenge_id] = self.api.getAlgorithms(self.challenge_id)\n",
        "          cloneOrPullTIGRepo(\"challenges\")\n",
        "          time.sleep(0.5)\n",
        "\n",
        "          self.status = \"Picking algorithm to benchmark\"\n",
        "          self.algorithm_id = self.pickAlgorithmToBenchmark()\n",
        "          time.sleep(0.5)\n",
        "\n",
        "          self.status = \"Querying difficulty frontiers\"\n",
        "          self.frontiers = self.api.getFrontiers(self.challenge_id)\n",
        "          time.sleep(0.5)\n",
        "\n",
        "          self.status = \"Picking difficulty to benchmark\"\n",
        "          self.difficulty = self.pickDifficultyToBenchmark()\n",
        "          time.sleep(0.5)\n",
        "\n",
        "          self.benchmark_start = datetime.now().astimezone()\n",
        "          self.benchmark_end = self.benchmark_start + timedelta(seconds=60)\n",
        "\n",
        "          benchmark_params = dict(\n",
        "              player_id=self.earnings[\"player_id\"],\n",
        "              block_id=self.latest_block[\"block_id\"],\n",
        "              prev_block_id=self.latest_block[\"prev_block_id\"],\n",
        "              challenge_id=self.challenge_id,\n",
        "              algorithm_id=self.algorithm_id,\n",
        "              difficulty=self.difficulty\n",
        "          )\n",
        "\n",
        "          self.status = \"Benchmarking\"\n",
        "          while datetime.now().astimezone() < self.benchmark_end and self.running:\n",
        "              self.generateAndSolveInstance(benchmark_params)\n",
        "\n",
        "          if len(self.proofs) > 0:\n",
        "              self.status = \"Submitting Benchmark\"\n",
        "              resp = self.api.submitBenchmark(**benchmark_params, nonces=[p['nonce'] for p in self.proofs])\n",
        "              sampled_nonces = set(resp[\"sampled_nonces\"])\n",
        "              self.api.submitProofs(\n",
        "                  resp[\"benchmark_id\"],\n",
        "                  proofs=[\n",
        "                      p for p in self.proofs\n",
        "                      if p['nonce'] in sampled_nonces\n",
        "                  ]\n",
        "              )\n",
        "          else:\n",
        "              self.status = \"Benchmark underperformed. Skipping submission\"\n",
        "              self.underperformed_benchmarks['data_rows'].append(\n",
        "                  (\n",
        "                    str(uuid4()),\n",
        "                    self.benchmark_end.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\"),\n",
        "                    0,\n",
        "                    self.latest_block['block_id'],\n",
        "                    self.challenge_id,\n",
        "                    self.algorithm_id,\n",
        "                    self.difficulty,\n",
        "                    0,\n",
        "                    None,\n",
        "                    'UNDER-PERFORMED',\n",
        "                  )\n",
        "              )\n",
        "              time.sleep(2)\n",
        "\n",
        "          while (\n",
        "              len(self.underperformed_benchmarks['data_rows']) > 0 and\n",
        "               (datetime.now().astimezone() - datetime.strptime(self.underperformed_benchmarks['data_rows'][0][0], \"%Y-%m-%dT%H:%M:%S.%f%z\")).total_seconds() > self.latest_block['seconds_benchmark_active']\n",
        "          ):\n",
        "              self.underperformed_benchmarks['data_rows'].pop(0)\n",
        "\n",
        "      def run_forever(self):\n",
        "          while self.running:\n",
        "              try:\n",
        "                  self.reset()\n",
        "                  self.run_once()\n",
        "              except Exception as e:\n",
        "                  self.status = f\"Error: {e}\"\n",
        "                  import traceback\n",
        "                  print(traceback.format_exc())\n",
        "                  print(e)\n",
        "                  time.sleep(5)\n",
        "\n",
        "  G[\"BENCHMARKER\"] = None\n",
        "\n",
        "\n",
        "# Start Flask Server if it doesn't exist\n",
        "if \"SERVER\" not in G:\n",
        "  from flask import Flask, request\n",
        "  from werkzeug.serving import make_server\n",
        "\n",
        "  app = Flask(\"SimpleBenchmarker\")\n",
        "\n",
        "  @app.route('/get_status', methods=['GET'])\n",
        "  def getStatus():\n",
        "    if G['BENCHMARKER'] is None:\n",
        "        return dict(status=\"Waiting for API Key\")\n",
        "    else:\n",
        "      b = G['BENCHMARKER']\n",
        "      return dict(\n",
        "            api_key=b.api.api_key,\n",
        "            status=b.status,\n",
        "            earnings=b.earnings,\n",
        "            recent_benchmarks=b.recent_benchmarks,\n",
        "            underperformed_benchmarks=b.underperformed_benchmarks,\n",
        "            benchmark_start=None if b.benchmark_start is None else b.benchmark_start.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\"),\n",
        "            benchmark_end=None if b.benchmark_end is None else b.benchmark_end.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\"),\n",
        "            challenge_id=b.challenge_id,\n",
        "            algorithm_id=b.algorithm_id,\n",
        "            difficulty=b.difficulty,\n",
        "      )\n",
        "\n",
        "  @app.route('/start_benchmarking/<api_key>', methods=['GET'])\n",
        "  def startBenchmark(api_key):\n",
        "    if G['BENCHMARKER'] is None or G['BENCHMARKER'].api.api__key != api_key:\n",
        "      G['BENCHMARKER'] = SimpleBenchmarker(\n",
        "          API(api_key, \"https://api.the-innovation-game.com/v1\")\n",
        "      )\n",
        "      Thread(target=G['BENCHMARKER'].run_forever).start()\n",
        "    return 'OK'\n",
        "\n",
        "  @app.route('/stop_benchmarking', methods=['GET'])\n",
        "  def stopBenchmarking():\n",
        "    if G['BENCHMARKER'] is not None:\n",
        "      G['BENCHMARKER'].running = False\n",
        "      G['BENCHMARKER'] = None\n",
        "    return 'OK'\n",
        "\n",
        "  G[\"SERVER\"] = make_server(\"0.0.0.0\", 3000, app)\n",
        "  Thread(target=G[\"SERVER\"].serve_forever).start()\n",
        "\n",
        "try:\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    import requests\n",
        "    clear_output(wait=True)\n",
        "    resp = requests.get(\"https://the-innovation-game.com/simple-benchmarker\")\n",
        "    resp.encoding = 'UTF-8'\n",
        "    display(HTML(resp.text))\n",
        "except ImportError as e:\n",
        "   logger.error(\"IPython module does not exist. This script is intended to be ran in a notebook\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
